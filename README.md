# Transformer-based-machine-translation-system

NLPè¯¾ç¨‹ä½œä¸šï¼šåŸºäºtransformerçš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ğŸ¤—

## Data

ä½œä¸šæ‰“åŒ…ç»™å‡ºï¼Œæ•°æ®é‡å¤§æ¦‚åœ¨ä¸¤ä¸‡å¤šæ¡ä¸­è‹±æ–‡è¯­å¥å¯¹ã€‚

## Model

- **åˆ†è¯å·¥å…·**ï¼š[SentencePiece](https://github.com/google/sentencepiece/)
- **Transformer**ï¼šå‚è€ƒHarvardå¼€æºçš„pytorchç‰ˆæœ¬ [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

## References

- Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep transformer models for machine translation. In Anna Korhonen, David Traum, and LluÃ­s MÃ rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810â€“1822, Florence, Italy, July 2019. Association for Computational Linguistics.
  - [Paper](https://aclanthology.org/P19-1176/)
